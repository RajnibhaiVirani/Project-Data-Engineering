version: '3.8'

# I am defining my microservices architecture here.
# I removed Kafka/Zookeeper to keep the system lightweight for my local machine,
# focusing on the Batch processing requirement using Spark and MinIO.

services:
  # 1. My Data Lake (Storage Layer)
  # I chose MinIO because it mimics AWS S3 but runs locally.
  minio_storage:
    image: minio/minio:latest
    container_name: minio_server
    ports:
      - "9000:9000" # API port
      - "9001:9001" # Console port
    environment:
      MINIO_ROOT_USER: "admin_user"
      MINIO_ROOT_PASSWORD: "secure_password"
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
    volumes:
      - minio_data:/data
    networks:
      - pipeline_network

  # 2. My Data Warehouse (Serving Layer)
  # Postgres is reliable for storing the final aggregated reports.
  postgres_warehouse:
    image: postgres:13
    container_name: postgres_db
    environment:
      POSTGRES_USER: "admin_user"
      POSTGRES_PASSWORD: "secure_password"
      POSTGRES_DB: "fraud_detection_db"
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./sql:/docker-entrypoint-initdb.d # This runs my schema.sql automatically
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin_user -d fraud_detection_db"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - pipeline_network

  # 3. Ingestion Service
  # This runs my Python script to move data from local to MinIO.
  ingestion_service:
    build: ./src/ingestion
    container_name: my_ingestion_script
    depends_on:
      minio_storage:
        condition: service_healthy
    volumes:
      - ./data:/app/data # Mounting my local data folder so the container can see it
    environment:
      MINIO_ENDPOINT: minio_storage:9000
      MINIO_ACCESS_KEY: admin_user
      MINIO_SECRET_KEY: secure_password
    networks:
      - pipeline_network

  # 4. Processing Service (Spark)
  # This runs the heavy lifting (PySpark) logic.
  processing_service:
    build: ./src/processing
    container_name: my_spark_job
    depends_on:
      minio_storage:
        condition: service_healthy
      postgres_warehouse:
        condition: service_healthy
      ingestion_service:
        condition: service_started
    environment:
      MINIO_ENDPOINT: minio_storage:9000
      MINIO_ACCESS_KEY: admin_user
      MINIO_SECRET_KEY: secure_password
      DATABASE_URL: postgresql://admin_user:secure_password@postgres_warehouse:5432/fraud_detection_db
    networks:
      - pipeline_network

  # 5. Airflow (Orchestration)
  # I included this to demonstrate quarterly scheduling capability.
  airflow:
    image: apache/airflow:2.6.3
    container_name: airflow_scheduler
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
    volumes:
      - ./airflow/dags:/opt/airflow/dags
    ports:
      - "8080:8080"
    networks:
      - pipeline_network

volumes:
  minio_data:
  pgdata:

networks:
  pipeline_network:
    driver: bridge