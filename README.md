# ðŸ§  DLMDSEDE02 â€“ Batch Data Engineering Pipeline  
## Fraud Detection Data Lake â†’ PySpark Processing â†’ PostgreSQL Warehouse

[![Docker](https://img.shields.io/badge/Docker-Compose-blue)](https://www.docker.com/)
[![Python 3.10+](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](https://www.python.org/)
[![Spark 3.4.1](https://img.shields.io/badge/Spark-3.4.1-orange)](https://spark.apache.org/)
[![License: Educational](https://img.shields.io/badge/license-Educational-lightgrey)](LICENSE)
[![GitHub last commit](https://img.shields.io/github/last-commit/YOUR_USERNAME/Project_Data_Engineering)]()

> A complete microservices-based batch processing pipeline using MinIO, PySpark, PostgreSQL, and Airflow, developed for IU assignment DLMDSEDE02.

---

## ðŸ“– Overview

This project implements a modular, containerized, end-to-end batch data engineering pipeline that ingests raw CSV files, stores them in a Data Lake, processes them using PySpark, and loads structured reports into a PostgreSQL Data Warehouse.

The components work together via Docker Compose, mirroring real-world industry data architectures.

Pipeline Stages:
1. Ingestion â€“ Uploads bankdataset.csv from local storage into MinIO (S3-compatible).
2. Processing â€“ PySpark performs data cleaning and calculates:
   - Daily domain transaction trends  
   - Location performance metrics  
   - Domain-level value leaderboard  
3. Loading â€“ Saves aggregated tables into PostgreSQL.
4. Orchestration â€“ Airflow DAG simulates quarterly scheduled runs.

---

## âœ¨ Key Features

| Feature | Description |
|--------|-------------|
| MinIO Data Lake | Local S3-like storage for raw CSV ingestion |
| PySpark Transformations | Batch job to clean, transform, and aggregate data |
| PostgreSQL Warehouse | Stores three analytical reporting tables |
| Resilient Services | Automatic retries using `tenacity` |
| Containerized Architecture | Fully isolated microservices with Docker Compose |
| Airflow Scheduling | Quarterly pipeline orchestration |
| Windows-Friendly | Works seamlessly on Windows with Docker Desktop |

---

## ðŸ—‚ï¸ Project Structure

Project_Data_Engineering/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ bankdataset.csv
â”‚
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ schema.sql
â”‚
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ quarterly_dag.py
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ ingest.py
â”‚   â”‚
â”‚   â””â”€â”€ processing/
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â””â”€â”€ spark_job.py
â”‚
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md

---

## âš™ï¸ Technologies Used

| Tool | Purpose |
|------|---------|
| Python 3.10+ | Ingestion and processing scripts |
| PySpark 3.4.1 | Distributed data transformations |
| MinIO | Local S3-based data lake |
| PostgreSQL 13 | Analytical data warehouse |
| Airflow 2.6 | Workflow orchestration |
| SQLAlchemy | DB connection and loading |
| Docker & Docker Compose | Containerized microservices |
| Tenacity | Retry logic for robustness |

---

## ðŸš€ How to Run the Pipeline

1. Ensure Docker Desktop is running  
2. Place your dataset:
   Project_Data_Engineering/data/bankdataset.csv

3. Build & run all services:
   docker-compose up --build

Expected Output:
- Ingestion â†’ â€œFile safely stored in MinIOâ€  
- Processing â†’ â€œReports are live in PostgreSQLâ€  

---

## ðŸ—„ï¸ Generated Warehouse Tables

rpt_daily_domain_trends â€“ Daily average value per domain  
rpt_location_performance â€“ Location-level averages  
rpt_domain_leaderboard â€“ Ranked total value by domain  

---

## ðŸ” Access Interfaces

MinIO Console â†’ http://localhost:9001  
Airflow UI â†’ http://localhost:8080  
Postgres (psql) â†’ admin_user / secure_password

---

## ðŸ”„ Workflow Diagram (Mermaid)

```mermaid
flowchart LR
    A[Local CSV File\n(bankdataset.csv)] --> B[Ingestion Service\n(Python + MinIO SDK)]
    B --> C[MinIO Data Lake]

    C --> D[Processing Service\n(PySpark)]
    D --> E1[Daily Domain Trends]
    D --> E2[Location Performance]
    D --> E3[Domain Leaderboard]

    E1 --> F[(PostgreSQL Warehouse)]
    E2 --> F
    E3 --> F

    subgraph Airflow
        G[Quarterly DAG]
    end

    G --> B
    G --> D
